{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Resume Scoring using NLP and ML \n",
    "\n",
    "to identify the most suitable candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 331 kB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
      "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61084 sha256=a863022c0b6f6a032b4a10df0aabb4f0261fa9b86185f09d83408c064d67b963\n",
      "  Stored in directory: /Users/macbookpro/Library/Caches/pip/wheels/b1/1a/8f/a4c34be976825a2f7948d0fa40907598d69834f8ab5889de11\n",
      "Successfully built PyPDF2\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3963 sha256=5a0729efae434a0da71dbd6b2aaa61ea72354908f4fa9b53c395ce6c67bec398\n",
      "  Stored in directory: /Users/macbookpro/Library/Caches/pip/wheels/55/f0/2c/81637d42670985178b77df6d41b9b6c6dc18c94818447414b9\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PyPDF2\n",
    "\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Azure\n",
      "\n",
      "Microsoft Azure\n",
      "\n",
      "Data Modeling\n",
      "\n",
      "Data Modeling\n",
      "\n",
      "Data Cleaning\n",
      "\n",
      "Data Cleaning\n",
      "\n",
      "Java\n",
      "\n",
      "Java\n",
      "\n",
      "HTML/CSS\n",
      "\n",
      "\n",
      "\n",
      "HTML/CSS\n",
      "\n",
      "\n",
      "\n",
      "Power Bi\n",
      "\n",
      "\n",
      "\n",
      "Power Bi\n",
      "\n",
      "\n",
      "\n",
      "Experience\n",
      "\n",
      "Experience\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "\n",
      "\n",
      "Junior data scientist\n",
      "\n",
      "Junior data scientist\n",
      "\n",
      "Computer Science graduate adept at collecting, analyzing, interpreting large datasets and performing data management tasks. Possessing an extensive analytical skill, strong attention to detail. Seeking for a Data Scientist position with a forward-moving company.\n",
      "\n",
      "\n",
      "\n",
      "Computer Science graduate adept at collecting, analyzing, interpreting large datasets and performing data management tasks. Possessing an extensive analytical skill, strong attention to detail. Seeking for a Data Scientist position with a forward-moving company.\n",
      "\n",
      "\n",
      "\n",
      "Tableau\n",
      "\n",
      "\n",
      "\n",
      "Tableau\n",
      "\n",
      "\n",
      "\n",
      "R studio\n",
      "\n",
      "R studio\n",
      "\n",
      "SQL\n",
      "\n",
      "SQL\n",
      "\n",
      "Python\n",
      "\n",
      "Python\n",
      "\n",
      "https://www.linkedin.com/in/manal-almehmadi\n",
      "\n",
      "\n",
      "\n",
      "https://www.linkedin.com/in/manal-almehmadi\n",
      "\n",
      "\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "\n",
      "\n",
      "mmehmadi94@gmail.com\n",
      "\n",
      "mmehmadi94@gmail.com\n",
      "\n",
      "+966 550 155 728\n",
      "\n",
      "+966 550 155 728\n",
      "\n",
      "Email\n",
      "\n",
      "Email\n",
      "\n",
      "PROFILE\n",
      "\n",
      "PROFILE\n",
      "\n",
      "CONTACT\n",
      "\n",
      "CONTACT\n",
      "\n",
      "Technical SKILLS\n",
      "\n",
      "Technical SKILLS\n",
      "\n",
      "Phone\n",
      "\n",
      "Phone\n",
      "\n",
      "Manal Almehmadi\n",
      "\n",
      "Manal Almehmadi\n",
      "\n",
      "Your adress tunisie  azertyazert azerty\n",
      "hhhhhhhhh\n",
      "\n",
      "Your adress tunisie  azertyazert azerty\n",
      "hhhhhhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI Engineer Intern\n",
      "\n",
      "Designing or modifying embedded systems that meet needs and requirements.\n",
      "\n",
      "Developing a real time face mask detector by openCV\n",
      "\n",
      "Building chatbots using IBM Watson and Tidio.\n",
      "\n",
      "Developing front-end pages to control robots.\n",
      "\n",
      "Successfully completed 8 tasks as Artificial Intelligence Engineer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Smart Methods\n",
      "\n",
      "AI Engineer Intern\n",
      "\n",
      "Designing or modifying embedded systems that meet needs and requirements.\n",
      "\n",
      "Developing a real time face mask detector by openCV\n",
      "\n",
      "Building chatbots using IBM Watson and Tidio.\n",
      "\n",
      "Developing front-end pages to control robots.\n",
      "\n",
      "Successfully completed 8 tasks as Artificial Intelligence Engineer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Smart Methods\n",
      "\n",
      "System Analyst\n",
      "\n",
      "Create and monitor Support Tickets Dashboard.\n",
      "\n",
      "Automate some manual business processes using Microsoft Flow.\n",
      "\n",
      "Follow up with consultants to ensure ERP issues are resolved on time and efficiently.\n",
      "\n",
      "Create a documentation guidelines for SCM-ERP system users.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Saudi Air Navigation Services (SANS)\n",
      "\n",
      "System Analyst\n",
      "\n",
      "Create and monitor Support Tickets Dashboard.\n",
      "\n",
      "Automate some manual business processes using Microsoft Flow.\n",
      "\n",
      "Follow up with consultants to ensure ERP issues are resolved on time and efficiently.\n",
      "\n",
      "Create a documentation guidelines for SCM-ERP system users.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Saudi Air Navigation Services (SANS)\n",
      "\n",
      "09/2019 – 04/2020\n",
      "\n",
      "09/2019 – 04/2020\n",
      "\n",
      "07/2018 – 08/2018\n",
      "\n",
      "07/2018 – 08/2018\n",
      "\n",
      "System Analyst Intern\n",
      "\n",
      "Perform UX analysis for a pre-launch mobile application.\n",
      "\n",
      "Assist in writing SRS documents.\n",
      "\n",
      "Conduct a Market Research to test viability of a new service\n",
      "\n",
      "\n",
      "\n",
      "Saudi Enaya Cooperative Insurance Company \n",
      "\n",
      "System Analyst Intern\n",
      "\n",
      "Perform UX analysis for a pre-launch mobile application.\n",
      "\n",
      "Assist in writing SRS documents.\n",
      "\n",
      "Conduct a Market Research to test viability of a new service\n",
      "\n",
      "\n",
      "\n",
      "Saudi Enaya Cooperative Insurance Company \n",
      "\n",
      "06/2020 – 08/2020\n",
      "\n",
      "06/2020 – 08/2020\n",
      "\n",
      "07/2017 – 08/2017\n",
      "\n",
      "07/2017 – 08/2017\n",
      "\n",
      "Web Developer (COOP)\n",
      "\n",
      "Develop a website to manage collage students’ summer training requests. The programming languages used to implement the website were ASP.NET, C# and SQL Server as database.\n",
      "\n",
      "Saudi Arabian Airline \n",
      "\n",
      "Web Developer (COOP)\n",
      "\n",
      "Develop a website to manage collage students’ summer training requests. The programming languages used to implement the website were ASP.NET, C# and SQL Server as database.\n",
      "\n",
      "Saudi Arabian Airline \n",
      "\n",
      "     \n",
      "\n",
      "Bachelor of Science in Computer Science\n",
      "\n",
      "King Abdulaziz University (KAU)\n",
      "\n",
      "GPA\n",
      "\n",
      "4.68 \n",
      "\n",
      "5\n",
      "\n",
      "     \n",
      "\n",
      "Bachelor of Science in Computer Science\n",
      "\n",
      "King Abdulaziz University (KAU)\n",
      "\n",
      "GPA\n",
      "\n",
      "4.68 \n",
      "\n",
      "5\n",
      "\n",
      "02/2014 – 06/2018\n",
      "\n",
      "02/2014 – 06/2018\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your adress tunisie  azertyazert azerty\n",
      "hhhhhhhhh\n",
      "\n",
      "Your adress tunisie  azertyazert azerty\n",
      "hhhhhhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Projects\n",
      "\n",
      "Projects\n",
      "\n",
      "Data Science Bootcamp\n",
      "\n",
      "Successfully completed 48 tasks and 13 projects during the bootcamp.\n",
      "\n",
      "\n",
      "\n",
      "Coding Dojo\n",
      "\n",
      "Data Science Bootcamp\n",
      "\n",
      "Successfully completed 48 tasks and 13 projects during the bootcamp.\n",
      "\n",
      "\n",
      "\n",
      "Coding Dojo\n",
      "\n",
      "Data Analysis Nanodegree\n",
      "\n",
      "Develop a website to manage collage students’ summer training requests. The programming languages used to implement the website were ASP.NET, C# and SQL Server as database.\n",
      "\n",
      "Udacity\n",
      "\n",
      "Data Analysis Nanodegree\n",
      "\n",
      "Develop a website to manage collage students’ summer training requests. The programming languages used to implement the website were ASP.NET, C# and SQL Server as database.\n",
      "\n",
      "Udacity\n",
      "\n",
      "07/2017 – 08/2017\n",
      "\n",
      "07/2017 – 08/2017\n",
      "\n",
      "07/2017 – 08/2017\n",
      "\n",
      "07/2017 – 08/2017\n",
      "\n",
      "Data Foundation Nanodegree\n",
      "\n",
      "Develop a website to manage collage students’ summer training requests. The programming languages used to implement the website were ASP.NET, C# and SQL Server as database.\n",
      "\n",
      "Udacity\n",
      "\n",
      "Data Foundation Nanodegree\n",
      "\n",
      "Develop a website to manage collage students’ summer training requests. The programming languages used to implement the website were ASP.NET, C# and SQL Server as database.\n",
      "\n",
      "Udacity\n",
      "\n",
      "\n",
      "\n",
      "08/2020 – Present\n",
      "\n",
      "08/2020 – Present\n"
     ]
    }
   ],
   "source": [
    "# Store the resume in in a variable\n",
    "\n",
    "\n",
    "resume = docx2txt.process(\"CV.docx\")\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The position holder is responsible for modeling complex business problems, through the use of advanced analytic skills. Also proficient at integrating and preparing large, varied datasets, architecting specialized database and computing environments, and communicating results. Other responsibilities include providing data that is congruent and reliable. The position holder will need to validate their findings using an experimental and iterative approach. Data should be turn into critical information and knowledge that can be used to make sound organizational decisions, According to the policies and procedures followed within ELM.\n",
      "\n",
      "\n",
      "\n",
      "Analysis and Recognition:\n",
      "\n",
      "\n",
      "\n",
      "Works with and alongside business analysts by suggesting other products of interest to the client. \n",
      "\n",
      "Models and frames business scenarios that are meaningful and which impact on critical business processes and/or decisions. \n",
      "\n",
      " Works in iterative processes with the client and validates findings \n",
      "\n",
      "Validates analysis using scenario modelling. \n",
      "\n",
      "Develops innovative and effective approaches to solve client's analytics problems and communicates results and methodologies. \n",
      "\n",
      "Develops experimental design approaches to validate finding or test hypotheses. Identifies/creates the appropriate algorithm to discover patterns\n",
      "\n",
      "\n",
      "\n",
      "Gathering and Data organizing:\n",
      "\n",
      "\n",
      "\n",
      "Identifies what data is available and relevant, including internal and external data sources, leveraging new data collection processes such as smart meters and geo-location information or social media. \n",
      "\n",
      " Collaborates with Institute subject matter experts to select the relevant sources of information. \n",
      "\n",
      "Makes strategic recommendations on data collection, integration and retention requirements incorporating business requirements and knowledge of best practices. \n",
      "\n",
      " Assesses, with the business, opportunities to enhance the qualification and assurance of the information to strengthen the use case. \n",
      "\n",
      "Defines the validity of the information, how long the information is meaningful, and what other information it is related to. \n",
      "\n",
      " Utilizes patterns and variations in the volume, speed and other characteristics of data supporting the initiative, the type of data (e.g., images, text, clickstream or metering data) in predictive analysis. \n",
      "\n",
      "Qualifies where information can be stored or what information, external to the organization, may be used in support of the use case.\n",
      "\n",
      "\n",
      "\n",
      "Deep Learning:\n",
      "\n",
      "\n",
      "\n",
      "Excellent knowledge in different deep learning practices (CNN, RNN, LTSM, Reinforcement Learning, etc) as well as Machine Learning in general\n",
      "\n",
      "Practical experience in implementing systems using deep learning\n",
      "\n",
      "Experience with one or more of the deep learning frameworks such as: Tensorflow, Caffe, Keras, PyTorch\n",
      "\n",
      "Experience in Computer Vision and object detection is preferred\n",
      "\n",
      "Contribution in research communities, publishing papers or participation in Github projects related to deep learning is preferred\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JOB SPECIFICATIONS \n",
      "\n",
      "\n",
      "\n",
      "Academic and professional qualifications \n",
      "\n",
      "\n",
      "\n",
      "Phd./Masters in Data Science OR AI OR Machine Learning OR Mathematics OR Statistics OR Computer science related field;\n",
      "\n",
      "\n",
      "\n",
      "Years and Nature of Experience :\n",
      "\n",
      "\n",
      "\n",
      "8-12 years of relevant experience in a related industry\n"
     ]
    }
   ],
   "source": [
    "# store the job desccription into a variable \n",
    "job_des = docx2txt.process(\"job_description.docx\")\n",
    "\n",
    "# print the job desccription\n",
    "print(job_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of text\n",
    "\n",
    "text = [resume, job_des]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "count_martix = cv.fit_transform(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " similarity scores: \n",
      "[[1.         0.49217538]\n",
      " [0.49217538 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# print the similarity scores\n",
    "print(\"\\n similarity scores: \")\n",
    "print(cosine_similarity(count_martix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your resume matches about 49.22% of the job description\n"
     ]
    }
   ],
   "source": [
    "# get the match percentage \n",
    "\n",
    "matchPercentage = cosine_similarity(count_martix)[0][1]*100\n",
    "matchPercentage = round(matchPercentage, 2) # round to two decimal places\n",
    "\n",
    "print(\"Your resume matches about \"+ str(matchPercentage)+\"% of the job description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF_MARKER = b'%%EOF'\n",
    "file_name = 'resumes/CV.pdf'\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# check if EOF is somewhere else in the file\n",
    "if EOF_MARKER in contents:\n",
    "    # we can remove the early %%EOF and put it at the end of the file\n",
    "    contents = contents.replace(EOF_MARKER, b'')\n",
    "    contents = contents + EOF_MARKER\n",
    "else:\n",
    "    # Some files really don't have an EOF marker\n",
    "    # In this case it helped to manually review the end of the file\n",
    "    print(contents[-8:]) # see last characters at the end of the file\n",
    "    # printed b'\\n%%EO%E'\n",
    "    contents = contents[:-6] + EOF_MARKER\n",
    "\n",
    "with open(file_name.replace('.pdf', '') + '_fixed.pdf', 'wb') as f:\n",
    "    f.write(contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qpdf --password= --decrypt CV.pdf output.pdf\n",
    "\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "\n",
    "with open(\"resumes/CV.pdf\", \"rb\") as in_file:\n",
    "    input_pdf = PdfFileReader(in_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pdf = PdfFileWriter()\n",
    "output_pdf.appendPagesFromReader(input_pdf)\n",
    "output_pdf.encrypt(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resumes/output.pdf\", \"wb\") as out_file:\n",
    "        output_pdf.write(out_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Phrase Matcher code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.3.4-cp38-cp38-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0; python_version >= \"3.6\"\n",
      "  Downloading blis-0.7.3-cp38-cp38-macosx_10_9_x86_64.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.18.5)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.4-cp38-cp38-macosx_10_9_x86_64.whl (263 kB)\n",
      "\u001b[K     |████████████████████████████████| 263 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.4-cp38-cp38-macosx_10_9_x86_64.whl (287 kB)\n",
      "\u001b[K     |████████████████████████████████| 287 kB 733 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.4-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.47.0)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.3-cp38-cp38-macosx_10_9_x86_64.whl (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 829 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.4-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Installing collected packages: catalogue, blis, wasabi, cymem, murmurhash, preshed, srsly, plac, thinc, spacy\n",
      "Successfully installed blis-0.7.3 catalogue-1.0.0 cymem-2.0.4 murmurhash-1.0.4 plac-1.1.3 preshed-3.0.4 spacy-2.3.4 srsly-1.0.4 thinc-7.4.3 wasabi-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%EOF` not found.\n"
     ]
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "%EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read resumes from the folder one by one\n",
    "mypath='resumes' #enter your path here where you saved the resumes\n",
    "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "\n",
    "def pdfextract(file):\n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "    countpage = fileReader.getNumPages()\n",
    "    count = 0\n",
    "    text = []\n",
    "    while count < countpage:    \n",
    "        pageObj = fileReader.getPage(count)\n",
    "        count +=1\n",
    "        t = pageObj.extractText()\n",
    "        print (t)\n",
    "        text.append(t)\n",
    "    return text\n",
    "\n",
    "#function to read resume ends\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def pdfextract(file):\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    c = collections.Counter(range(number_of_pages))\n",
    "    for i in c:\n",
    "        #page\n",
    "        page = read_pdf.getPage(i)\n",
    "        page_content = page.extractText()\n",
    "    return (page_content.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'!!!!2.;<-1=>\\n*I.5J%=6J<44%,5/747<%%\\nRPSTRTR!U!\"$#.#+6\\n!RVSTRWV!U!RPSTRWV\\n!&\\'(\\'%)*+,-*,%.//(*\\'01\\n%*+,,-../+001&,2340-5-6&%)&57.8.&796&$:&4;2<-,5.&6+;=9>&5?-&@225,734\\nA&&!\"#$%&\\'(\")\"\\n\\'&\\'(\\'%2-\\'345+5%6\\'-/7,89,,\\n%B-C-024&7&D-@.=5-&52&3797>-&,2007>-&.5+6-95.E&.+33-;&5;7=9=9>&;-F+-.5.A&G?-&\\n4;2>;733=9>&079>+7>-.&+.-6&52&=340-3-95&5?-&D-@.=5-&D-;-&H*IAJKGL&MN&796&\\n*OP&*-;C-;&7.&6757@7.-A\\n&*#+,$-.\\n\\'&\\'(\\'%:/;-7\\'(+/-%6\\'-/7,89,,\\n%B-C-024&7&D-@.=5-&52&3797>-&,2007>-&.5+6-95.E&.+33-;&5;7=9=9>&;-F+-.5.A&G?-&\\n4;2>;733=9>&079>+7>-.&+.-6&52&=340-3-95&5?-&D-@.=5-&D-;-&H*IAJKGL&MN&796&\\n*OP&*-;C-;&7.&6757@7.-A\\n&*#+,$-.\\n\\'RVSTRWV!U!RPSTRWV\\n!'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfextract('resumes/CV.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that does phrase matching and builds a candidate profile\n",
    "def create_profile(file):\n",
    "    text = pdfextract(file) \n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    keyword_dict = pd.read_csv('template_new.xlsx')\n",
    "   # stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "   # NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "    ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "   # DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "    #R_words = [nlp(text) for text in keyword_dict['R Language'].dropna(axis = 0)]\n",
    "    #python_words = [nlp(text) for text in keyword_dict['Python Language'].dropna(axis = 0)]\n",
    "    # Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "  #  matcher.add('Stats', None, *stats_words)\n",
    "  #  matcher.add('NLP', None, *NLP_words)\n",
    "    matcher.add('ML', None, *ML_words)\n",
    "   # matcher.add('DL', None, *DL_words)\n",
    "    #matcher.add('R', None, *R_words)\n",
    "    #matcher.add('Python', None, *python_words)\n",
    "    #matcher.add('DE', None, *Data_Engineering_words)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "        span = doc[start : end]  # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
    "    \n",
    "    ## convertimg string of keywords to dataframe\n",
    "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "    \n",
    "    base = os.path.basename(file)\n",
    "    filename = os.path.splitext(base)[0]\n",
    "       \n",
    "    name = filename.split('_')\n",
    "    name2 = name[0]\n",
    "    name2 = name2.lower()\n",
    "    ## converting str to dataframe\n",
    "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "    \n",
    "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "\n",
    "    return(dataf)\n",
    "        \n",
    "#function ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "PdfReadError",
     "evalue": "EOF marker not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPdfReadError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-113dfb13e245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monlyfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monlyfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfinal_database\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-5938fcd0dbdc>\u001b[0m in \u001b[0;36mcreate_profile\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#function that does phrase matching and builds a candidate profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdfextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-95344ed4db04>\u001b[0m in \u001b[0;36mpdfextract\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpdfextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfileReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcountpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/PyPDF2/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, strict, warndest, overwriteWarnings)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/PyPDF2/pdf.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mb_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%%EOF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlast1K\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EOF marker not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNextEndLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  line:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPdfReadError\u001b[0m: EOF marker not found"
     ]
    }
   ],
   "source": [
    "#code to execute/call the above functions\n",
    "\n",
    "final_database=pd.DataFrame()\n",
    "i = 0 \n",
    "while i < len(onlyfiles):\n",
    "    file = onlyfiles[i]\n",
    "    dat = create_profile(file)\n",
    "    final_database = final_database.append(dat)\n",
    "    i +=1\n",
    "    print(final_database)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4a0f78e42c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "   \n",
    "#code to count words under each category and visulaize it through Matplotlib\n",
    "\n",
    "final_database2 = final_database['Keyword'].groupby([final_database['Candidate Name'], final_database['Subject']]).count().unstack()\n",
    "final_database2.reset_index(inplace = True)\n",
    "final_database2.fillna(0,inplace=True)\n",
    "new_data = final_database2.iloc[:,1:]\n",
    "new_data.index = final_database2['Candidate Name']\n",
    "#execute the below line if you want to see the candidate profile in a csv format\n",
    "#sample2=new_data.to_csv('sample.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "ax = new_data.plot.barh(title=\"Resume keywords by category\", legend=False, figsize=(25,7), stacked=True)\n",
    "labels = []\n",
    "for j in new_data.columns:\n",
    "    for i in new_data.index:\n",
    "        label = str(j)+\": \" + str(new_data.loc[i][j])\n",
    "        labels.append(label)\n",
    "patches = ax.patches\n",
    "for label, rect in zip(labels, patches):\n",
    "    width = rect.get_width()\n",
    "    if width > 0:\n",
    "        x = rect.get_x()\n",
    "        y = rect.get_y()\n",
    "        height = rect.get_height()\n",
    "        ax.text(x + width/2., y + height/2., label, ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
