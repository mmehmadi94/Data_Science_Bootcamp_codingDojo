{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "AKGfyxWhAJui"
   },
   "source": [
    "# *k*-means clustering\n",
    "\n",
    "Up until now, we have only examined *supervised* machine learning (ML), in which we provide labeled training data for the algorithm. *k*-means clustering is the first *unsupervised* ML algorithm that we have dealt with: we provide the algorithm with a set of $n$ data points and tell the algorithm that we want that data partitioned into $k$ clusters that provide the best separation between clusters. \n",
    "\n",
    "The *k*-means algorithm randomly assigns $k$ *centroids* (or geometric centers of subsets of the dataset as measured through [Euclidean distance](http://mathworld.wolfram.com/Distance.html)) to the data and then iteratively moves those centroids around the feature space until it converges on an arrangement of the centroids the minimizes the variance within the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal|20%](Images/K-means_convergence.gif \"segment\")\n",
    "\n",
    "(Image courtesy of [Wikimedia Commons](https://en.wikipedia.org/wiki/File:K-means_convergence.gif) and is distributed under the terms of the [GNU Free Documentation License](https://en.wikipedia.org/wiki/GNU_Free_Documentation_License).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Learning objective:** By the end of this section, you should have a basic understanding of the kinds of results the *k*-means algorithm can produce and have some idea about how you can interpret those results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data\n",
    "\n",
    "To illustrate *k*-means clustering in action, we'll use the familiar [U.S. Department of Agriculture National Nutrient Database for Standard Reference](https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/nutrient-data-laboratory/docs/usda-national-nutrient-database-for-standard-reference/) dataset.(Note that the path name is case sensitive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data/USDA-nndb-combined.csv', encoding='latin_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "6j8BELX6AJu4",
    "outputId": "494cbab7-0d6a-48b0-cb43-ce188b98dc30"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZou0fWjS2sy"
   },
   "source": [
    "Because the *k*-means algorithm works with the Euclidean distance between data points, we need to once again separate out our descriptive from our numeric features (while saving the descriptive features for later use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df = df.iloc[:, [0, 1, 2]+[i for i in range(50,54)]]\n",
    "desc_df.set_index('NDB_No', inplace=True)\n",
    "desc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutr_df = df.iloc[:, :-5]\n",
    "nutr_df = nutr_df.drop(['FoodGroup', 'Shrt_Desc'], axis=1)\n",
    "nutr_df.set_index('NDB_No', inplace=True)\n",
    "nutr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations that we have seen before in this dataset are still an issue here.\n",
    "\n",
    "|   | column            | row               | corr |\n",
    "|--:|------------------:|------------------:|-----:|\n",
    "| 0 | Folate\\_Tot\\_(µg) | Folate\\_DFE\\_(µg) | 0.98 |\n",
    "| 1 | Folic\\_Acid\\_(µg) | Folate\\_DFE\\_(µg) | 0.95 |\n",
    "| 2 | Folate\\_DFE\\_(µg) | Folate\\_Tot\\_(µg) | 0.98 |\n",
    "| 3 | Vit\\_A\\_RAE       | Retinol\\_(µg)     | 0.99 |\n",
    "| 4 | Retinol\\_(µg)     | Vit\\_A\\_RAE       | 0.99 |\n",
    "| 5 | Vit\\_D\\_µg        | Vit\\_D\\_IU        | 1    |\n",
    "| 6 | Vit\\_D\\_IU        | Vit\\_D\\_µg        | 1    |\n",
    "\n",
    "Before dropping anything from the `df` `DataFrame`, however, let's take a quick look at these correlations visually. Let's start with `Folate_Tot_(µg)` and `Folate_DFE_(µg)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "SByR5hmPAJvU",
    "outputId": "620fb81d-86ca-4a58-e730-2d9471b375aa"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(df['Folate_Tot_(µg)'], df['Folate_DFE_(µg)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dramatic correlation!\n",
    "\n",
    "> **Troubleshooting note:** Despite specifying Latin_1 encoding when we read in the dataset from the CSV file, character corruption can still occur. If the plt.scatter call above fails, try this code instead: `plt.scatter(df['Folate\\_Tot\\_(Âµg)'], df['Folate\\_DFE\\_(Âµg)'])`\n",
    "\n",
    "> **Exercise**\n",
    ">\n",
    "> In the code cell below, run a scatterplot of another correlative pair of features from this dataset. (**Hint:** You only need to run the `plt.scatter` function with new parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop `Folate_DFE_(µg)`, `Vit_A_RAE`, and `Vit_D_IU` from `df` in order to eliminate these problematic correlations. The *k*-means algorithm also doesn't work with `NaN` values, so we will also have to drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutr_df.drop(['Folate_DFE_(µg)', 'Vit_A_RAE', 'Vit_D_IU'], \n",
    "        inplace=True, axis=1)\n",
    "nutr_df = nutr_df.dropna()\n",
    "nutr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Troubleshooting note:** Again, if the read_csv() function did not read in the µ (mu) symbol correctly and the drop() method call above fails, try this code instead: `nutr_df.drop(['Folate\\_DFE\\_(Âµg)', 'Vit\\_A\\_RAE', 'Vit\\_D\\_IU'], inplace=True, axis=1)`\n",
    "\n",
    "Because the *k*-means algorithm will be calculating the Euclidean distances between data points and centroids, we need to ensure that all of the numeric features in our dataset use compatible units; we don't want to try and calculate distances between units of mass like grams and units of energy like kilocalories. As we will with principal component analysis (PCA), we will use scikit-learn's [`StandardScaler()` function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to center the data around each feature's mean and transform each feature to have a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hq82GRHDAJvy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(nutr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9AKLeZiP5_4"
   },
   "source": [
    "## Fitting the *k*-means clustering model\n",
    "\n",
    "We are now ready to import *k*-means clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_0y0tL6AJv6"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Uz5zE4fP2hI"
   },
   "source": [
    "While we can use any number of clusters that we wish, we will initially use three clusters as a convenient number for an initial exploration of k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-q1AMqGAJwC"
   },
   "outputs": [],
   "source": [
    "kmeansmodel = KMeans(n_clusters=3, random_state=13)\n",
    "kmeansmodel.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCDOaubpPzkF"
   },
   "source": [
    "> **Question**\n",
    ">\n",
    "> Given what you have learned in this course thus far, what role does our declaration of the `random_state` above play in the `KMeans()` function?\n",
    "\n",
    "Let's take a look at the labels for our clusters.\n",
    "\n",
    "> **Question**\n",
    ">\n",
    "> Given what you know about Python zero-indexing and the number of clusters we selected, what labels do you expect to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "D7R__0ozAJwS",
    "outputId": "8e8c6e87-9971-40fb-8b3d-1f3e247dfa46"
   },
   "outputs": [],
   "source": [
    "kmeansmodel.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zS4MIorsROGa"
   },
   "source": [
    "As expected, our labels for the different clusters are `0`, `1`, and `2`.\n",
    "\n",
    "Let's examine the distribution of data points among the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "j8rF_9nFAJwZ",
    "outputId": "156ddbf3-f065-4e82-adb7-cbf642f76a87"
   },
   "outputs": [],
   "source": [
    "pd.value_counts(kmeansmodel.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85gUpwR4SPlx"
   },
   "source": [
    "You might also check various cluster stats for comparison of the clusters.\n",
    "\n",
    "We will want to access these labels later on, so let's add them to our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "kURLg3i3TVud",
    "outputId": "840c84e3-5af7-4565-9fbe-14201da5f4be"
   },
   "outputs": [],
   "source": [
    "nutr_df['Cluster'] = kmeansmodel.labels_\n",
    "nutr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Takeaway:** Because data naturally clusters, we should not expect the clusters to be the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the *k*-means clustering model\n",
    "\n",
    "What sorts of foods populate the different clusters? We'll examine that question in greater depth later on, but we can get some sense of our clusters just by looking at the mean values for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "2QGRzJ8-AJwu",
    "outputId": "bd7387e4-7c90-4be4-ffe9-548b8440afee"
   },
   "outputs": [],
   "source": [
    "nutr_df.groupby('Cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly see that cluter 2 encompasses fatty foods (high `Lipid_Tot_(g)` values). Clusters 0 and 1 have similar mean protein and lipid amounts; an easy-to-see differentiation between those clusters is their relative carbohydrate and sugar levels, with those in cluster 1 being significantly higher than those in cluster 0.\n",
    "\n",
    "We can look at these high-level differences in a little more detail using the `describe()` method, though, honestly, given the size of the `DataFrame`, this is a little cumbersome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "iWNsLalrSqCF",
    "outputId": "d1e0bbaa-d0cc-4989-f349-fb3e50178c90"
   },
   "outputs": [],
   "source": [
    "nutr_df.groupby('Cluster').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyKQF5-CXxR8"
   },
   "source": [
    "We can also predict the label for new or hypothetical observations. Let's provide the nutritional details for Gjetost cheese and see which cluster our model places it in. (Gjetost cheese is one of our discarded entries from the original `DataFrame` with `NaN` values imputed below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcase = [[1021,\n",
    "            13.44,\n",
    "            466.0,\n",
    "            9.65,\n",
    "            29.51,\n",
    "            4.75,\n",
    "            42.65,\n",
    "            0.0,\n",
    "            1.50,\n",
    "            400.0,\n",
    "            0.52,\n",
    "            70.0,\n",
    "            444.0,\n",
    "            1409.0,\n",
    "            600.0,\n",
    "            1.14,\n",
    "            0.08,\n",
    "            0.04,\n",
    "            14.5,\n",
    "            0.0,\n",
    "            0.315,\n",
    "            1.382,\n",
    "            0.813,\n",
    "            3.351,\n",
    "            0.271,\n",
    "            5.0,\n",
    "            0.0,\n",
    "            5.0,\n",
    "            15.4,\n",
    "            2.42,\n",
    "            1113.0,\n",
    "            334.0,\n",
    "            0,\n",
    "            20,\n",
    "            0,\n",
    "            0.5,\n",
    "            22,\n",
    "            2.4,\n",
    "            19.16,\n",
    "            7.879,\n",
    "            0.938,\n",
    "            94.0,\n",
    "            28.35]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yo_2TROLAJxH",
    "outputId": "afc98b71-f4ac-4b9b-a002-dd42c3eac92d"
   },
   "outputs": [],
   "source": [
    "kmeansmodel.predict(newcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSAmIauFZajK"
   },
   "source": [
    "The output indicates that Gjetost cheese falls under cluster 1.\n",
    "\n",
    "> **Exercise**\n",
    ">\n",
    "> Now go back up and alter the values for Gjetost cheese  in the `newcase` array above to see what it takes to get that array classified into cluster 0 or cluster 2.\n",
    "\n",
    "We can also see the actual coordinate values for cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4y5N1dswAJxZ",
    "outputId": "b8901890-19ce-4cba-c5eb-52e384000a05"
   },
   "outputs": [],
   "source": [
    "kmeansmodel.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there is a method to this wall of numbers. Each coordinate is a point in 43-dimensional space, so each centroid is represented by an array of 43 values.\n",
    "\n",
    "Let's now add back in our descriptive features so that we can see which food groups are in our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = nutr_df.join(desc_df)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the value counts for the different food groups in our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[nutr_df['Cluster'] == 0]['FoodGroup'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**\n",
    ">\n",
    "> Look back over the Python in the code cell above. Does the syntax make sense? If not, review the documentation for [`pandas.DataFrame.loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) and [`pandas.Series.value_counts`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html).\n",
    "\n",
    "> **Exercise**\n",
    ">\n",
    "> Now find the `FoodGroup` value counts for clusters 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the entries in our clusters makes sense and others look kind of like the contents of a grab bag. Part of the reason for this is that the *k*-means algorithm has to draw rather arbitrary boundaries between clusters; there will be a lot of entries in all clusters that are right on the edge and could reasonably belong to two (or more) clusters. To reduce some of this noise, we can sort these by distance from centroid of the respective clusters and look at the entries closest to the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "distances = kmeansmodel.fit_transform(X)\n",
    "merged_df['Distance'] = np.min(distances, axis=1)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Questions**\n",
    ">\n",
    "> 1. Why is it only necessary to find the minimum distance to the three centroids for a given data point (`np.min`)?\n",
    "> 2. What role does the `axis=1` parameter play in the `np.min()` function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sort `FoodGroup` value counts by distance to the cluster's centroid, which can make the lists of principal food types in each cluster more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[nutr_df['Cluster'] == 0].sort_values(by='Distance')['FoodGroup'][:500].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**\n",
    ">\n",
    "> Look back over the Python in the code cell above. Does the syntax make sense? If not, review the documentation for [`pandas.DataFrame.sort_values`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) and [slicing ranges in pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges).\n",
    "\n",
    "> **Exercise**\n",
    ">\n",
    "> Now find the sorted `FoodGroup` value counts for clusters 1 and 2. (**Note:** Due to the differing sizes of the clusters, try a variety of ranges for your slices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Takeaway:** Even without subject-matter expertise, we can gain insights of varying depth about the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKjocvLxZvx2"
   },
   "source": [
    "## Visualizing the *k*-means clustering model\n",
    "\n",
    "Similar to the situation with PCA, we can create visualizations of our clusters, but visualization is crude at best when it has been projected down through so many dimensions. That said, even trying to project from 43 dimensions down to 3 dimensions, noticeable patterns persist. To perform this visualization, we will first need to import the Axes3D module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = Axes3D(fig)\n",
    "C = kmeansmodel.cluster_centers_\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=kmeansmodel.labels_, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Takeaway:** Two observations:\n",
    "> 1. Notice that there is no hard boundary visible between the purple and yellow clusters. The boundary is there, but, because it's a higher-dimensional hyperplane, it gets smeared across three dimensional space.\n",
    "> 2. Note also that points from the light blue cluster are scattered across the plot. This is also an artifact of the clusters projection down from 43 dimensions to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the number of clusters to use\n",
    "\n",
    "You might have noticed that we manually determined the number $k$, the number of clusters, earlier in this section. We assigned `n_clusters_ = 3` in the model. But is 3 the right number of clusters for this dataset?\n",
    "\n",
    "In practice, determining the correct number of clusters can require a lot on domain knowledge and data interpretation. However, we can use the elbow method as a rough heuristic for determining the number of clusters to use, even absent any domain knowledge.\n",
    "\n",
    "In the elbow method, we plot the within-cluster sum of squares (WCSS, a measure of the variability of the observations within each cluster) against different values of $k$. Similar to the scree graph we used with PCA, we are looking for the value of $k$ near where WCSS begins to flatten out and form an elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QvMLOk6JAJx0"
   },
   "outputs": [],
   "source": [
    "WCSS = []\n",
    "for num in range(1, 12):\n",
    "    kmeans = KMeans(n_clusters = num,  random_state = 50)\n",
    "    kmeans.fit(X)\n",
    "    WCSS.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "0xUik8gBezxu",
    "outputId": "c5524b0b-4092-4003-cd3d-b435fcd04f10"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 8))\n",
    "    \n",
    "plt.plot(range(1, 12), WCSS,'ro--',linewidth=2, markersize=12)\n",
    "# plt.title('ELBOW METHOD')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4z8JhU9pgwGm"
   },
   "source": [
    "> **Takeaway:** Ideally, we would want to see a more pronounced elbow. One could make the case for the elbow being at $k=5$ — or at $k=8$ or $k=10$. And some datasets do produce a distinct elbow. However, real-world data is inherently messy, and there is no perfect way to evaluate *k*-means models apart from domain expertise and rigorous analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running k-means clustering outside of Jupyter notebooks\n",
    "\n",
    "Python code does not have to reside in notebooks. Simple programs can provide a useful tool for investigating your data. To see this in action, open the program [k-Means.py](https://github.com/daniel-dc-cd/data_science/blob/master/daily_materials/ml_kmeans_nb_regression/k-Means.py) in the GitHub repository for this course.\n",
    "\n",
    "The program is largely the code from earlier in this notebook. Now run k-Means.py from the command line interface on your local computer. It will ask for the number of clusters and the file path for the USDA dataset that you have been using in this section.\n",
    "\n",
    "> **Exercise**\n",
    ">\n",
    "> Run k-Means.py using several different numbers for k (including those indicated by the WCSS graph above). Do they provide intuitive groupings of food groups?\n",
    "\n",
    "> **Takeaway:** One reason for Python’s popularity in the data-science community is its extreme flexibility. It provides numerous tools that data scientists can use in a variety of ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *k*-means and PCA\n",
    "\n",
    "What if we went with $k=5$ instead of $k=3$? Would the contents of our clusters align with the components we identified in the [PCA Notebook?]() We did, after all, divide the National Nutrient Database into five components in that section using PCA.\n",
    "\n",
    "It's a seductive idea, but an incorrect one. Let's examine *why* it's incorrect; we'll start by re-fitting our *k*-means model on five clusters. (Remember that we defined earlier in this section `X = StandardScaler().fit_transform(nutr_df)`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, random_state=1)\n",
    "km.fit(X)\n",
    "nutr_df['Cluster'] = km.labels_\n",
    "nutr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = km.fit_transform(X)\n",
    "merged_df = nutr_df.join(desc_df)\n",
    "merged_df['Distance'] = np.min(dists, axis=1)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined `merged_df` for $k=5$. Let's now extract the two features we will want to combine with our PCA results: `Cluster` and `FoodGroup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = merged_df[['Cluster', 'FoodGroup']]\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run PCA on this dataset and join `Cluster` and `FoodGroup` to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fit = PCA()\n",
    "pca = fit.fit_transform(X)\n",
    "pca_df = pd.DataFrame(pca[:, :5], index=nutr_df.index)\n",
    "pca_df = pca_df.join(cluster_df)\n",
    "pca_df.rename(columns={0:'c1', 1:'c2', 2:'c3', 3:'c4', 4:'c5'}, \n",
    "              inplace=True)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the PCA results in a `DataFrame`, let's sort them by $c_1$ and see how the cluster labels break down among the top 500 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.sort_values(by='c1')['Cluster'][:500].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c_1$ proved to have pretty close overlap with cluster 2. What about $c_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.sort_values(by='c2')['Cluster'][:500].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the results are decidedly more mixed. But you can get a sense of how *k*-means clusters and PCA components do (and don't) overlap by changing the number of entries in the slice.\n",
    "\n",
    "> **Exercise**\n",
    ">\n",
    "> Evaluate the extent to which *k*-means clusters overlap with PCA components for $c_3$, $c_4$, and $c_5$. You might also want to adjust the size of slices that you take (more or less than 500) in order to see what effect that has on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Takeaway:** While there can be some overlap between PCA and *k*-means, you should never conflate the two analytical techniques in your mind. *k*-means partitions a dataset into discrete clusters while PCA identifies components of the feature space of a dataset that convey the most information about the structure of the dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "block1_kmeans1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
